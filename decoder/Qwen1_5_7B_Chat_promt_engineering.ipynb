{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "q_2Fa66ExghV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import accelerate\n",
        "print(accelerate.__version__)"
      ],
      "metadata": {
        "id": "pTX3w1TqyFrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_promts(seed1, seed2, seed3, seed0, promt_shuffle_type, num_prompt_examples):\n",
        "  df_raw_processed = pd.read_csv('raw_processed.csv')\n",
        "\n",
        "  np.random.seed(seed1)\n",
        "  random_numbers_positive = np.random.randint(0, 2500000/2, size=60)\n",
        "  np.random.seed(seed2)\n",
        "  random_numbers_negative = np.random.randint(2500000/2, 2500000, size=60)\n",
        "\n",
        "  prompt_task = \"Predict the emotion of a tweet. Answer with a single word: positive or negative\\n\"\n",
        "  ## prompt_task = \"What is the emotion of the following tweet? Please answer with a single word: positive or negative\\n\"\n",
        "  ## prompt_examples = prompt_task + \"Examples:\\n\"\n",
        "  prompt_examples = \"Examples of how to predict the emotion of tweets:\\n\"\n",
        "  count_positive = 0\n",
        "  count_negative = 0\n",
        "  array = None\n",
        "  if promt_shuffle_type == 0:\n",
        "    array = np.array([0]*num_prompt_examples + [1]*num_prompt_examples)\n",
        "  if promt_shuffle_type == 1 or promt_shuffle_type ==2:\n",
        "    array = np.array([1]*num_prompt_examples + [0]*num_prompt_examples)\n",
        "  if promt_shuffle_type == 2:\n",
        "    np.random.seed(seed3)\n",
        "    np.random.shuffle(array)\n",
        "  for i in range(len(array)):\n",
        "    if array[i]==1:\n",
        "      prompt_examples = prompt_examples + \"{}) Tweet: \".format(i+1) + str(df_raw_processed.iloc[random_numbers_positive[count_positive]]['text']) + \"\\nAnswer: positive\\n\"\n",
        "      count_positive += 1\n",
        "    else:\n",
        "      prompt_examples = prompt_examples + \"{}) Tweet: \".format(i+1) + str(df_raw_processed.iloc[random_numbers_negative[count_negative]]['text']) + \"\\nAnswer: negative\\n\"\n",
        "      count_negative += 1\n",
        "\n",
        "  prompt_examples += \"Predict the sentiment of the following tweet. Answer with only a single word: positive or negative\\n\"\n",
        "  df_raw_processed = pd.read_csv('raw_processed.csv')\n",
        "  prompts = []\n",
        "  prompts_ground_truth = []\n",
        "  count = 0\n",
        "  np.random.seed(seed0)\n",
        "  random_numbers = np.random.randint(0, 2500000, size=1220)\n",
        "  for i in range(1220):\n",
        "    if count == 100:\n",
        "      break\n",
        "    if random_numbers[i] in random_numbers_positive or random_numbers[i] in random_numbers_negative:\n",
        "      continue\n",
        "    new_prompt = prompt_examples + \"Tweet: \" + str(df_raw_processed.iloc[random_numbers[i]]['text']) + \"\\nAnswer: \"\n",
        "    prompts.append(new_prompt)\n",
        "    prompts_ground_truth.append((lambda label: 'positive' if label == 1 else 'negative')(df_raw_processed.iloc[random_numbers[i]]['labels']))\n",
        "    count += 1\n",
        "\n",
        "  return prompts, prompts_ground_truth"
      ],
      "metadata": {
        "id": "r3hY3l_LMMIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts_list = []\n",
        "number_of_examples = 11\n",
        "prompts1, prompts_ground_truth = get_promts(85653, 8551, 8752, 42, 1, number_of_examples)\n",
        "prompts2, prompts_ground_truth = get_promts(5577, 325, 9843, 42, 2, number_of_examples)\n",
        "prompts3, prompts_ground_truth = get_promts(32329, 12843, 6326, 42, 2, number_of_examples)\n",
        "prompts4, prompts_ground_truth = get_promts(5910, 2665, 1234, 42, 2, number_of_examples)\n",
        "prompts5, prompts_ground_truth = get_promts(13947, 54573, 2024, 42, 2, number_of_examples)\n",
        "prompts6, prompts_ground_truth = get_promts(357, 9827, 1391, 42, 2, number_of_examples)\n",
        "prompts7, prompts_ground_truth = get_promts(171, 43699, 2875, 42, 2, number_of_examples)\n",
        "prompts8, prompts_ground_truth = get_promts(253, 853, 123592, 42, 2, number_of_examples)\n",
        "prompts_list.append(prompts1)\n",
        "prompts_list.append(prompts2)\n",
        "prompts_list.append(prompts3)\n",
        "prompts_list.append(prompts4)\n",
        "prompts_list.append(prompts5)\n",
        "prompts_list.append(prompts6)\n",
        "prompts_list.append(prompts7)\n",
        "prompts_list.append(prompts8)"
      ],
      "metadata": {
        "id": "eN6aJctKMO9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4Lgc5aeuSTS"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import accelerate\n",
        "device = \"cuda\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen1.5-7B-Chat\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-7B-Chat\")\n",
        "\n",
        "prompt = prompts1[0]\n",
        "\n",
        "prompts_collection = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "text = tokenizer.apply_chat_template(prompts_collection, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "samples = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "samples_generated = model.generate(samples.input_ids, max_new_tokens=512, do_sample=True)\n",
        "\n",
        "samples_generated = [output_ids[len(input_ids):] for input_ids, output_ids in zip(samples.input_ids, samples_generated)]\n",
        "\n",
        "response = tokenizer.batch_decode(samples_generated, skip_special_tokens=True)[0]\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "responses = []\n",
        "number_of_responses = len(prompts1)\n",
        "count_correct = 0\n",
        "\n",
        "for i, prompt in enumerate(prompts1):\n",
        "  if (i % 10 == 0):\n",
        "    print(i)\n",
        "  prompts_collection = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "  text = tokenizer.apply_chat_template(prompts_collection, tokenize=False, add_generation_prompt=True)\n",
        "  samples = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "  samples = {k: v.to(device) for k, v in samples.items()}\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model.generate(samples[\"input_ids\"],\n",
        "                              attention_mask=samples[\"attention_mask\"],\n",
        "                              max_new_tokens=5,\n",
        "                              do_sample=True,\n",
        "                              return_dict_in_generate=True,\n",
        "                              output_scores=False)\n",
        "\n",
        "\n",
        "\n",
        "  samples_generated = [output_ids[len(input_ids):] for input_ids, output_ids in zip(samples[\"input_ids\"], outputs.sequences)]\n",
        "  response = tokenizer.batch_decode(samples_generated, skip_special_tokens=True)[0]\n",
        "\n",
        "  response = response.replace('.', '').lower()\n",
        "\n",
        "  if response == prompts_ground_truth[i]:\n",
        "      count_correct += 1\n",
        "\n",
        "  responses.append(response)\n",
        "  del samples\n",
        "  torch.cuda.empty_cache()\n",
        "print(\"Accuracy: {:.4f}%\".format(float(count_correct / number_of_responses)))\n",
        "\n"
      ],
      "metadata": {
        "id": "HF7BovhZlnP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disclaimer:\n",
        "Code contains refactored and reused code from:\n",
        "\n",
        "\n",
        "*   https://huggingface.co/Qwen/Qwen1.5-7B-Chat\n",
        "\n",
        "\n",
        "*   https://qwenlm.github.io/blog/qwen1.5/\n",
        "*   https://github.com/QwenLM/Qwen2\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wzaqzoROcF4z"
      }
    }
  ]
}