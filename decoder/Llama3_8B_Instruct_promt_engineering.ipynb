{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "q_2Fa66ExghV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import accelerate\n",
        "print(accelerate.__version__)"
      ],
      "metadata": {
        "id": "pTX3w1TqyFrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_promts(seed1, seed2, seed3, seed0, promt_shuffle_type, num_prompt_examples):\n",
        "  df_raw_processed = pd.read_csv('raw_processed.csv')\n",
        "\n",
        "  np.random.seed(seed1)\n",
        "  random_numbers_positive = np.random.randint(0, 2500000/2, size=60)\n",
        "  np.random.seed(seed2)\n",
        "  random_numbers_negative = np.random.randint(2500000/2, 2500000, size=60)\n",
        "  prompt_task = \"Predict the emotion of a tweet. Answer with a single word: positive or negative\\n\"\n",
        "  ## prompt_task = \"What is the sentiment of the following tweet? Please answer with a single word: positive or negative\\n\"\n",
        "  ## prompt_examples = prompt_task + \"Examples:\\n\"\n",
        "  prompt_examples = \"Examples of how to predict the emotion of tweets:\\n\"\n",
        "  count_positive = 0\n",
        "  count_negative = 0\n",
        "  array = None\n",
        "  if promt_shuffle_type == 0:\n",
        "    array = np.array([0]*num_prompt_examples + [1]*num_prompt_examples)\n",
        "  if promt_shuffle_type == 1 or promt_shuffle_type ==2:\n",
        "    array = np.array([1]*num_prompt_examples + [0]*num_prompt_examples)\n",
        "  if promt_shuffle_type == 2:\n",
        "    np.random.seed(seed3)\n",
        "    np.random.shuffle(array)\n",
        "  for i in range(len(array)):\n",
        "    if array[i]==1:\n",
        "      prompt_examples = prompt_examples + \"{}) Tweet: \".format(i+1) + str(df_raw_processed.iloc[random_numbers_positive[count_positive]]['text']) + \"\\nAnswer: positive\\n\"\n",
        "      count_positive += 1\n",
        "    else:\n",
        "      prompt_examples = prompt_examples + \"{}) Tweet: \".format(i+1) + str(df_raw_processed.iloc[random_numbers_negative[count_negative]]['text']) + \"\\nAnswer: negative\\n\"\n",
        "      count_negative += 1\n",
        "\n",
        "  prompt_examples += \"Predict the emotion of the following tweet. Answer with only a single word: positive or negative\\n\"\n",
        "  df_raw_processed = pd.read_csv('raw_processed.csv')\n",
        "  prompts = []\n",
        "  prompts_ground_truth = []\n",
        "  count = 0\n",
        "  np.random.seed(seed0)\n",
        "  random_numbers = np.random.randint(0, 2500000, size=1220)\n",
        "  for i in range(1220):\n",
        "    if count == 100:\n",
        "      break\n",
        "    if random_numbers[i] in random_numbers_positive or random_numbers[i] in random_numbers_negative:\n",
        "      continue\n",
        "    new_prompt = prompt_examples + \"Tweet: \" + str(df_raw_processed.iloc[random_numbers[i]]['text']) + \"\\nAnswer: \"\n",
        "    prompts.append(new_prompt)\n",
        "    prompts_ground_truth.append((lambda label: 'positive' if label == 1 else 'negative')(df_raw_processed.iloc[random_numbers[i]]['labels']))\n",
        "    count += 1\n",
        "    if count == 1:\n",
        "      print(new_prompt)\n",
        "  return prompts, prompts_ground_truth"
      ],
      "metadata": {
        "id": "Gs0NVYsxt5OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts_list = []\n",
        "number_of_examples = 11\n",
        "prompts1, prompts_ground_truth = get_promts(85653, 8551, 8752, 42, 2, number_of_examples)\n",
        "prompts2, prompts_ground_truth = get_promts(5577, 325, 9843, 42, 2, number_of_examples)\n",
        "prompts3, prompts_ground_truth = get_promts(32329, 12843, 6326, 42, 2, number_of_examples)\n",
        "prompts4, prompts_ground_truth = get_promts(5910, 2665, 1234, 42, 2, number_of_examples)\n",
        "prompts5, prompts_ground_truth = get_promts(13947, 54573, 2024, 42, 2, number_of_examples)\n",
        "prompts6, prompts_ground_truth = get_promts(357, 9827, 1391, 42, 2, number_of_examples)\n",
        "prompts7, prompts_ground_truth = get_promts(171, 43699, 2875, 42, 2, number_of_examples)\n",
        "prompts8, prompts_ground_truth = get_promts(253, 853, 123592, 42, 2, number_of_examples)\n",
        "prompts_list.append(prompts1)\n",
        "prompts_list.append(prompts2)\n",
        "prompts_list.append(prompts3)\n",
        "prompts_list.append(prompts4)\n",
        "prompts_list.append(prompts5)\n",
        "prompts_list.append(prompts6)\n",
        "prompts_list.append(prompts7)\n",
        "prompts_list.append(prompts8)\n"
      ],
      "metadata": {
        "id": "VhnIV79gfih3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4Lgc5aeuSTS"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import accelerate\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "hf_token = \"hf_...\" ## paste your token here\n",
        "login(hf_token)\n",
        "device = \"cuda\"\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
        "\n",
        "\n",
        "prompt = prompts1[0]\n",
        "\n",
        "\n",
        "prompts_collection = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "\n",
        "text = tokenizer.apply_chat_template(prompts_collection, tokenize=False, add_generation_prompt=True)\n",
        "samples = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "\n",
        "samples_generated = model.generate(samples.input_ids, max_new_tokens=512, do_sample=True)\n",
        "samples_generated = [output_ids[len(input_ids):] for input_ids, output_ids in zip(samples.input_ids, samples_generated)]\n",
        "\n",
        "\n",
        "response = tokenizer.batch_decode(samples_generated, skip_special_tokens=True)[0]\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "responses_list = []\n",
        "\n",
        "for j, prompts in enumerate(prompts_list):\n",
        "  if tokenizer.pad_token is None:\n",
        "      tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  model.to(device)\n",
        "\n",
        "  responses = []\n",
        "  number_of_responses = len(prompts)\n",
        "  count_correct = 0\n",
        "\n",
        "  for i, prompt in enumerate(prompts):\n",
        "      if (i % 10 == 0):\n",
        "          print(i)\n",
        "\n",
        "      prompts_collection = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "      text = tokenizer.apply_chat_template(prompts_collection, tokenize=False, add_generation_prompt=True)\n",
        "      samples = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "      samples = {k: v.to(device) for k, v in samples.items()}\n",
        "\n",
        "      with torch.no_grad():\n",
        "          outputs = model.generate(\n",
        "              samples[\"input_ids\"],\n",
        "              attention_mask=samples[\"attention_mask\"],\n",
        "              max_new_tokens=10,\n",
        "              do_sample=True,\n",
        "              return_dict_in_generate=True,\n",
        "              output_scores=False,\n",
        "              pad_token_id=tokenizer.pad_token_id\n",
        "          )\n",
        "\n",
        "      samples_generated = [output_ids[len(input_ids):] for input_ids, output_ids in zip(samples[\"input_ids\"], outputs.sequences)]\n",
        "      response = tokenizer.batch_decode(samples_generated, skip_special_tokens=True)[0]\n",
        "\n",
        "      response = response.replace('.', '').lower()\n",
        "\n",
        "      if response == prompts_ground_truth[i]:\n",
        "          count_correct += 1\n",
        "\n",
        "      responses.append(response)\n",
        "      del samples\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "  responses_list.append(responses)\n",
        "  print(\"Accuracy: {:.4f}%\".format(float(count_correct / number_of_responses)))"
      ],
      "metadata": {
        "id": "QaJkrlH2eR4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_correct = 0.0\n",
        "accuracy_list = [0.73, 0.75, 0.76, 0.74, 0.71, 0.7, 0.62, 0.69]\n",
        "\n",
        "response_ensemble = []\n",
        "\n",
        "for i in range(100):\n",
        "  tmp_count = 0\n",
        "  for j, responses in enumerate(responses_list):\n",
        "\n",
        "    if responses[i] == 'positive':\n",
        "      tmp_count += 1.0\n",
        "    if responses[i] == 'negative':\n",
        "      tmp_count -= 1.0\n",
        "  if tmp_count > 0:\n",
        "    response_ensemble.append('positive')\n",
        "  elif tmp_count < 0:\n",
        "    response_ensemble.append('negative')\n",
        "  else:\n",
        "    response_ensemble.append('neutral')\n",
        "  if response_ensemble[i] == prompts_ground_truth[i]:\n",
        "    count_correct += 1\n",
        "number_of_responses = len(response_ensemble)\n",
        "print(\"Accuracy: {:.4f}%\".format(float(count_correct / number_of_responses)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7_WTDjMjOQH",
        "outputId": "b128b8fa-5741-420e-a23a-c5f68215cf63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7600%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disclaimer:\n",
        "Code may contain refactored and reused code from:\n",
        "\n",
        "\n",
        "*   https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n",
        "\n",
        "\n",
        "*   https://github.com/meta-llama/llama3"
      ],
      "metadata": {
        "id": "hnfzLZVWcj1B"
      }
    }
  ]
}